{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd40c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ace4facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFastaData(fasta_path):\n",
    "\n",
    "    fasta_file = fasta_path\n",
    "    sequences = []\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        sequences.append({\n",
    "            \"id\": record.id,\n",
    "            \"sequence\": str(record.seq),\n",
    "        })\n",
    "    result = []\n",
    "\n",
    "    for sequence in sequences:\n",
    "        \n",
    "        result.append(sequence[\"sequence\"])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b65ae9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = getFastaData(r'C:\\Users\\Admin\\Desktop\\self trans\\data\\pos_fasta_lysine.fasta')\n",
    "n = getFastaData(r'C:\\Users\\Admin\\Desktop\\self trans\\data\\neg_fasta_lysine.fasta')\n",
    "p_label = [1 for i in range(len(p))]\n",
    "n_label = [0 for i in range(len(n))]\n",
    "feature = p + n\n",
    "label = p_label + n_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e45e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "anji = []\n",
    "for fe in feature:\n",
    "    for it in fe:\n",
    "        if it not in anji and it != ' ':\n",
    "            anji.append(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd95c8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFre(fe):\n",
    "    temp_dict = {}\n",
    "    for aj in anji:\n",
    "        temp_dict[aj] = 0\n",
    "    for j in range(len(fe)):\n",
    "        if fe[j] == ' ':\n",
    "            continue\n",
    "        temp_dict[fe[j]] += 1\n",
    "    return [temp_dict[k] for k in anji]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11b83b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, AutoTokenizer,BertModel, AutoModel \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "class myModel(torch.nn.Module):\n",
    "    def __init__(self,esm2):\n",
    "        super(myModel,self).__init__()\n",
    "        self.esm2 = esm2\n",
    "        self.fc1 = torch.nn.Linear(1024,16)\n",
    "        self.fc2 = torch.nn.Linear(16,1)\n",
    "        \n",
    "        self.fc31 = torch.nn.Linear(1024,256)\n",
    "        self.fc32 = torch.nn.Linear(256,128)\n",
    "        \n",
    "        self.fc41 = torch.nn.Linear(1024,64)\n",
    "        self.fc42 = torch.nn.Linear(64,128)\n",
    "        \n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU() \n",
    "        self.bn1 = torch.nn.BatchNorm1d(128) \n",
    "        self.dropout1 = nn.Dropout(p=0.1)\n",
    "        self.dropout2 = nn.Dropout(p=0.1)\n",
    "    def forward(self,x):\n",
    "        outputs_ = self.esm2(**inputs)\n",
    "\n",
    "        x = outputs_.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout1(x)\n",
    "        x1 = self.fc1(x)\n",
    "        x1 = self.relu(x1)\n",
    "        \n",
    "        x2 = self.fc31(x)\n",
    "        x2 = self.relu(x2)\n",
    "        x2 = self.fc32(x2)\n",
    "        x2 = self.relu(x2)\n",
    "        \n",
    "        x3 = self.fc41(x)\n",
    "        x3 = self.relu(x3)\n",
    "        x3 = self.fc42(x3)\n",
    "        x3 = self.relu(x3)\n",
    "        \n",
    "        x = x1 + x2 + x3\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63de7f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, matthews_corrcoef, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def comp_result(y_test, y_pred, y_proba):\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "    print(f\"MCC: {mcc:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    \n",
    "    return accuracy,f1,auc,mcc,recall,precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ed53aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def kmer_composition(seq, k):\n",
    "    kmers = [seq[i:i+k] for i in range(len(seq)-k+1)]\n",
    "    kmer_counts = Counter(kmers)\n",
    "    return kmer_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11362151",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "kmer_counts_list = []\n",
    "for fe in feature:\n",
    "    kmer_counts = kmer_composition(fe, k)\n",
    "    for it in kmer_counts:\n",
    "        if it not in kmer_counts_list:\n",
    "            kmer_counts_list.append(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "980f7c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer_counts_dict = {}\n",
    "for i in range(len(kmer_counts_list)):\n",
    "    kmer_counts_dict[kmer_counts_list[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77a80d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, AutoTokenizer,BertModel, AutoModel \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_weights = F.softmax(self.W(x), dim=1)\n",
    "        output = attn_weights * x\n",
    "        return output\n",
    "\n",
    "class myModel(torch.nn.Module):\n",
    "    def __init__(self,esm2):\n",
    "        super(myModel,self).__init__()\n",
    "        self.esm2 = esm2\n",
    "        self.fc1 = torch.nn.Linear(1280,32)\n",
    "        self.fc2 = torch.nn.Linear(64,16)\n",
    "        self.fc3 = torch.nn.Linear(16,1)\n",
    "    \n",
    "        self.att1 = Attention(64)\n",
    "        self.att2 = Attention(16)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()  # 隐藏层的激活函数\n",
    "        self.bn1 = torch.nn.BatchNorm1d(128)  # 添加 Batch Normalization 层\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.fc_anji1 = torch.nn.Linear(420,64)\n",
    "        self.fc_anji2 = torch.nn.Linear(64,32)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.fz_fc1 = torch.nn.Linear(32,1)\n",
    "        self.fz_fc3 = torch.nn.Linear(32,1)\n",
    "        \n",
    "    def forward(self,x,batch_input_anji):\n",
    "        outputs_ = self.esm2(**inputs)\n",
    "\n",
    "        x = outputs_.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        fx_1 = self.fz_fc3(x)\n",
    "        fx_1 = self.sigmoid(fx_1)\n",
    "        \n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        batch_input_anji = self.fc_anji1(batch_input_anji)\n",
    "        batch_input_anji = self.relu(batch_input_anji)\n",
    "        batch_input_anji = self.fc_anji2(batch_input_anji)\n",
    "        batch_input_anji = self.relu(batch_input_anji)\n",
    "        \n",
    "        fx_2 = self.fz_fc1(batch_input_anji)\n",
    "        fx_2 = self.sigmoid(fx_2)\n",
    "        \n",
    "        batch_input_anji = self.dropout2(batch_input_anji)\n",
    "        \n",
    "        x = torch.cat((batch_input_anji, x), dim=1)\n",
    "\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x,fx_1,fx_2\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19649c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     2     3 ... 13946 13947 13948]\n",
      "kf 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import copy as cp\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "kf_in = 0\n",
    "\n",
    "feature = np.array(feature)\n",
    "label = np.array(label)\n",
    "\n",
    "for train_index, test_index in stratified_kfold.split(feature, label):\n",
    "    \n",
    "    save_model = None\n",
    "    best_accuracy = 0\n",
    "    best_f1 = 0\n",
    "    best_auc = 0\n",
    "    best_mcc = 0\n",
    "    best_recall = 0\n",
    "    best_precision = 0\n",
    "    \n",
    "\n",
    "    best_test_labels_com = None\n",
    "    best_predict_labels_com = None\n",
    "    best_test_outputs_com = None\n",
    "    best_intermediate_outputs = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(train_index)\n",
    "    kf_in += 1\n",
    "    print('kf',kf_in)\n",
    "    train_features, test_features = list(feature[train_index]), list(feature[test_index])\n",
    "    train_labels, test_labels = list(label[train_index]), list(label[test_index])\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "    cache_directory = r\"C:\\Users\\Admin\\Desktop\\neuropeptide prediction\\model\"\n",
    "\n",
    "\n",
    "    tokenizer_ = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\", cache_dir=cache_directory)\n",
    "    esm2 = AutoModel.from_pretrained(\"facebook/esm2_t33_650M_UR50D\", cache_dir=cache_directory)\n",
    "    esm2 = esm2.to(device)\n",
    "    del esm2.encoder.layer[4:]\n",
    "    \n",
    "    model = myModel(esm2)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "    \n",
    "    data_loader = DataLoader(list(zip(train_features, train_labels)), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "    test_data_loader = DataLoader(list(zip(test_features,test_labels)), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    num_epochs = 50\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_input, batch_labels in data_loader:\n",
    "        \n",
    "            batch_input_list = list(batch_input)\n",
    "            \n",
    "            batch_input_anji = []\n",
    "            for i in range(len(batch_input_list)):\n",
    "                batch_input_anji.append(getFre(batch_input_list[i]))\n",
    "                \n",
    "            for i in range(len(batch_input_list)):\n",
    "                kmer_counts = kmer_composition(batch_input_list[i], k)\n",
    "                feature_temp = [0 for i in range(400)]\n",
    "                for it in kmer_counts:\n",
    "                    feature_temp[kmer_counts_dict[it]] = kmer_counts[it]\n",
    "                batch_input_anji[i].extend(feature_temp)\n",
    "    \n",
    "            batch_input_anji = torch.tensor(batch_input_anji).float()\n",
    "            batch_input_anji = batch_input_anji.to(device)\n",
    "            \n",
    "                \n",
    "            inputs = tokenizer_(batch_input, return_tensors='pt', padding=True, truncation=True)\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs,batch_input_anji)\n",
    "        \n",
    "            outputs1 = outputs[0].view(-1)\n",
    "            outputs2 = outputs[1].view(-1)\n",
    "            outputs3 = outputs[2].view(-1)\n",
    "            loss1 = criterion(outputs1.to('cpu'), batch_labels.to(torch.float32))\n",
    "            loss2 = criterion(outputs2.to('cpu'), batch_labels.to(torch.float32))\n",
    "            loss3 = criterion(outputs3.to('cpu'), batch_labels.to(torch.float32))\n",
    "            loss = loss1 + loss2*0.3 + loss3*0.3\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        model.eval()  \n",
    "        test_loss = 0\n",
    "        test_loss1 = 0\n",
    "        test_loss2 = 0\n",
    "        test_loss3 = 0\n",
    "        test_loss4 = 0\n",
    "        correct_predictions1= 0\n",
    "        correct_predictions2= 0\n",
    "        correct_predictions3= 0\n",
    "        correct_predictions4= 0\n",
    "        \n",
    "        test_labels_com = []\n",
    "        predict_labels_com = []\n",
    "        test_outputs_com = []\n",
    "        intermediate_outputs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for test_batch_input,test_batch_labels in test_data_loader:\n",
    "                \n",
    "                batch_input_list = list(test_batch_input)\n",
    "            \n",
    "                batch_input_anji = []\n",
    "                for i in range(len(batch_input_list)):\n",
    "                    batch_input_anji.append(getFre(batch_input_list[i]))\n",
    "\n",
    "                    \n",
    "                for i in range(len(batch_input_list)):\n",
    "                    kmer_counts = kmer_composition(batch_input_list[i], k)\n",
    "                    feature_temp = [0 for i in range(400)]\n",
    "                    for it in kmer_counts:\n",
    "                        feature_temp[kmer_counts_dict[it]] = kmer_counts[it]\n",
    "                    batch_input_anji[i].extend(feature_temp)\n",
    "                batch_input_anji = torch.tensor(batch_input_anji).float()\n",
    "                batch_input_anji = batch_input_anji.to(device)\n",
    "                \n",
    "\n",
    "                inputs = tokenizer_(test_batch_input, return_tensors='pt', padding=True, truncation=True)\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "                test_outputs = model(inputs,batch_input_anji)\n",
    "                \n",
    "        \n",
    "                test_outputs1 = test_outputs[0].view(-1)\n",
    "                test_outputs2 = test_outputs[1].view(-1)\n",
    "                test_outputs3 = test_outputs[2].view(-1)\n",
    "        \n",
    "                \n",
    "        \n",
    "                loss1 = criterion(test_outputs1.to('cpu'), test_batch_labels.to(torch.float32)).item()\n",
    "                loss2 = criterion(test_outputs2.to('cpu'), test_batch_labels.to(torch.float32)).item()\n",
    "                loss3 = criterion(test_outputs3.to('cpu'), test_batch_labels.to(torch.float32)).item()\n",
    "                test_loss1 += loss1\n",
    "                test_loss2 += loss2\n",
    "                test_loss3 += loss3\n",
    "                \n",
    "                test_loss += (loss1 + loss2*0.3 + loss3*0.3)\n",
    "                \n",
    "                predictions1 = (test_outputs1 > 0.5).float() \n",
    "                predictions2 = (test_outputs2 > 0.5).float() \n",
    "                predictions3 = (test_outputs3 > 0.5).float() \n",
    "                \n",
    "                correct_predictions1 += (predictions1.to('cpu') == test_batch_labels).sum().item()\n",
    "                correct_predictions2 += (predictions2.to('cpu') == test_batch_labels).sum().item()\n",
    "                correct_predictions3 += (predictions3.to('cpu') == test_batch_labels).sum().item()\n",
    "                \n",
    "                predict_labels_com.extend(predictions1.tolist())\n",
    "                test_labels_com.extend(test_batch_labels.tolist())\n",
    "                test_outputs_com.extend(test_outputs1.tolist())\n",
    "        \n",
    "        average_test_loss = test_loss / len(test_data_loader)\n",
    "        average_test_loss1 = test_loss1 / len(test_data_loader)\n",
    "        average_test_loss2 = test_loss2 / len(test_data_loader)\n",
    "        average_test_loss3 = test_loss3 / len(test_data_loader)\n",
    "        accuracy1 = correct_predictions1 / len(test_feature)\n",
    "        accuracy2 = correct_predictions2 / len(test_feature)\n",
    "        accuracy3 = correct_predictions3 / len(test_feature)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        predict_labels_com = np.array(predict_labels_com)\n",
    "        test_labels_com = np.array(test_labels_com)\n",
    "        test_outputs_com = np.array(test_outputs_com)\n",
    "        temp_accuracy,temp_f1,temp_auc,temp_mcc,temp_recall,temp_precision = comp_result(test_labels_com,predict_labels_com,test_outputs_com)\n",
    "        if temp_accuracy > best_accuracy:\n",
    "            best_accuracy = temp_accuracy\n",
    "            best_f1 = temp_f1\n",
    "            best_auc = temp_auc\n",
    "            best_mcc = temp_mcc\n",
    "            best_recall = temp_recall\n",
    "            best_precision = temp_precision\n",
    "\n",
    "            \n",
    "            best_test_labels_com = test_labels_com\n",
    "            best_predict_labels_com = predict_labels_com\n",
    "            best_test_outputs_com = test_outputs_com\n",
    "            best_intermediate_outputs = intermediate_outputs\n",
    "            save_model = cp.deepcopy(model)\n",
    "            \n",
    "            temp_output_pd = pd.DataFrame(best_test_outputs_com)\n",
    "            temp_label_pd = pd.DataFrame(best_test_labels_com)\n",
    "\n",
    "        \n",
    "        average_train_loss = total_loss / len(data_loader)\n",
    "\n",
    "        model.train()\n",
    "        print('Epoch [{}/{}], Train Loss: {:.4f}, Test Loss: {:.4f}, Test Loss1: {:.4f}, Test Loss2: {:.4f}, Test Loss3: {:.4f}, Accuracy1: {:.2f}, Accuracy2: {:.2f}, Accuracy3: {:.2f}'.format(\n",
    "            epoch + 1, num_epochs, average_train_loss, average_test_loss, average_test_loss1, average_test_loss2, average_test_loss3, accuracy1 * 100, accuracy2 * 100, accuracy3 * 100))\n",
    "    print('=====================================')\n",
    "    print('kf',kf_in,\"best result:\")\n",
    "    print('acc: {:.4f}'.format(best_accuracy))\n",
    "    print('f1: {:.4f}'.format(best_f1))\n",
    "    print('auc： {:.4f}'.format(best_auc))\n",
    "    print('mcc: {:.4f}'.format(best_mcc))\n",
    "    print('recall：{:.4f}'.format(best_recall))\n",
    "    print('prec: {:.4f}'.format(best_precision))\n",
    "    print('=====================================')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
