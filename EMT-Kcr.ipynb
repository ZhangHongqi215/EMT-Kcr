{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd40c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "import os\n",
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ace4facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 获取FASTA\n",
    "def getFastaData(fasta_path):\n",
    "    # 定义FASTA文件路径\n",
    "    fasta_file = fasta_path\n",
    "\n",
    "    # 读取FASTA文件\n",
    "    sequences = []\n",
    "    for record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "        sequences.append({\n",
    "            \"id\": record.id,\n",
    "            \"sequence\": str(record.seq),\n",
    "        })\n",
    "    result = []\n",
    "    # 打印读取到的序列\n",
    "    for sequence in sequences:\n",
    "        \n",
    "        result.append(sequence[\"sequence\"])\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b65ae9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = getFastaData(r'C:\\Users\\Admin\\Desktop\\self trans\\data\\pos_fasta_lysine.fasta')\n",
    "n = getFastaData(r'C:\\Users\\Admin\\Desktop\\self trans\\data\\neg_fasta_lysine.fasta')\n",
    "p_label = [1 for i in range(len(p))]\n",
    "n_label = [0 for i in range(len(n))]\n",
    "feature = p + n\n",
    "label = p_label + n_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd3b77f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_p = getFastaData(r'C:\\Users\\Admin\\Desktop\\self trans\\data\\pos_ind_testdata.fasta')\n",
    "test_n = getFastaData(r'C:\\Users\\Admin\\Desktop\\self trans\\data\\neg_ind_testdata.fasta')\n",
    "test_p_label = [1 for i in range(len(test_p))]\n",
    "test_n_label = [0 for i in range(len(test_n))]\n",
    "test_feature = test_p + test_n\n",
    "test_label = test_p_label + test_n_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e45e02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "anji = []\n",
    "for fe in feature:\n",
    "    for it in fe:\n",
    "        if it not in anji and it != ' ':\n",
    "            anji.append(it)\n",
    "# amino_acids = dict()\n",
    "# for i in range(len(anji)):\n",
    "#     amino_acids[anji[i]] = i\n",
    "# test_feature_one = []\n",
    "# for i in range(len(test_feature)):\n",
    "#     temp = []\n",
    "#     for j in range(31):\n",
    "#         temp_2 = [0 for i in range(20)]\n",
    "#         temp_2[amino_acids[test_feature[i][j]]] = 1\n",
    "#         temp.extend(temp_2)\n",
    "#     test_feature_one.append(temp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bec98dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# anji = []\n",
    "# for fe in feature:\n",
    "#     for it in fe:\n",
    "#         if it not in anji and it != ' ':\n",
    "#             anji.append(it)\n",
    "# amino_acids = dict()\n",
    "# for i in range(len(anji)):\n",
    "#     amino_acids[anji[i]] = i\n",
    "# feature_one = []\n",
    "# for i in range(len(feature)):\n",
    "#     temp = []\n",
    "#     for j in range(31):\n",
    "#         temp_2 = [0 for i in range(20)]\n",
    "#         temp_2[amino_acids[feature[i][j]]] = 1\n",
    "#         temp.extend(temp_2)\n",
    "#     feature_one.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd95c8fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getFre(fe):\n",
    "    temp_dict = {}\n",
    "    for aj in anji:\n",
    "        temp_dict[aj] = 0\n",
    "    for j in range(len(fe)):\n",
    "        if fe[j] == ' ':\n",
    "            continue\n",
    "        temp_dict[fe[j]] += 1\n",
    "    return [temp_dict[k] for k in anji]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f30fe2be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# amino_acids = dict()\n",
    "# for i in range(len(anji)):\n",
    "#     amino_acids[anji[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "11b83b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, AutoTokenizer,BertModel, AutoModel \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "class myModel(torch.nn.Module):\n",
    "    def __init__(self,esm2):\n",
    "        super(myModel,self).__init__()\n",
    "        self.esm2 = esm2\n",
    "        self.fc1 = torch.nn.Linear(1024,16)\n",
    "        self.fc2 = torch.nn.Linear(16,1)\n",
    "        \n",
    "        self.fc31 = torch.nn.Linear(1024,256)\n",
    "        self.fc32 = torch.nn.Linear(256,128)\n",
    "        \n",
    "        self.fc41 = torch.nn.Linear(1024,64)\n",
    "        self.fc42 = torch.nn.Linear(64,128)\n",
    "        \n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()  # 隐藏层的激活函数\n",
    "        self.bn1 = torch.nn.BatchNorm1d(128)  # 添加 Batch Normalization 层\n",
    "        self.dropout1 = nn.Dropout(p=0.1)\n",
    "        self.dropout2 = nn.Dropout(p=0.1)\n",
    "    def forward(self,x):\n",
    "        outputs_ = self.esm2(**inputs)\n",
    "\n",
    "        x = outputs_.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout1(x)\n",
    "        x1 = self.fc1(x)\n",
    "        x1 = self.relu(x1)\n",
    "        \n",
    "        x2 = self.fc31(x)\n",
    "        x2 = self.relu(x2)\n",
    "        x2 = self.fc32(x2)\n",
    "        x2 = self.relu(x2)\n",
    "        \n",
    "        x3 = self.fc41(x)\n",
    "        x3 = self.relu(x3)\n",
    "        x3 = self.fc42(x3)\n",
    "        x3 = self.relu(x3)\n",
    "        \n",
    "        x = x1 + x2 + x3\n",
    "        x = self.bn1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "63de7f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, matthews_corrcoef, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def comp_result(y_test, y_pred, y_proba):\n",
    "    # 计算评估指标\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_proba)\n",
    "    mcc = matthews_corrcoef(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "\n",
    "    # 打印结果\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "    print(f\"MCC: {mcc:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    \n",
    "    return accuracy,f1,auc,mcc,recall,precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ed53aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "# 统计K-mer组成\n",
    "def kmer_composition(seq, k):\n",
    "    kmers = [seq[i:i+k] for i in range(len(seq)-k+1)]\n",
    "    kmer_counts = Counter(kmers)\n",
    "    return kmer_counts\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "11362151",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 2\n",
    "kmer_counts_list = []\n",
    "for fe in feature:\n",
    "    kmer_counts = kmer_composition(fe, k)\n",
    "    for it in kmer_counts:\n",
    "        if it not in kmer_counts_list:\n",
    "            kmer_counts_list.append(it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "980f7c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmer_counts_dict = {}\n",
    "for i in range(len(kmer_counts_list)):\n",
    "    kmer_counts_dict[kmer_counts_list[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77a80d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, AutoTokenizer,BertModel, AutoModel \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.W = nn.Linear(input_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn_weights = F.softmax(self.W(x), dim=1)\n",
    "        output = attn_weights * x\n",
    "        return output\n",
    "\n",
    "class myModel(torch.nn.Module):\n",
    "    def __init__(self,esm2):\n",
    "        super(myModel,self).__init__()\n",
    "        self.esm2 = esm2\n",
    "        self.fc1 = torch.nn.Linear(1280,32)\n",
    "        self.fc2 = torch.nn.Linear(64,16)\n",
    "        self.fc3 = torch.nn.Linear(16,1)\n",
    "    \n",
    "        self.att1 = Attention(64)\n",
    "        self.att2 = Attention(16)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()  # 隐藏层的激活函数\n",
    "        self.bn1 = torch.nn.BatchNorm1d(128)  # 添加 Batch Normalization 层\n",
    "        self.dropout1 = nn.Dropout(p=0.2)\n",
    "        self.dropout2 = nn.Dropout(p=0.5)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.fc_anji1 = torch.nn.Linear(420,64)\n",
    "        self.fc_anji2 = torch.nn.Linear(64,32)\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.fz_fc1 = torch.nn.Linear(32,1)\n",
    "        self.fz_fc3 = torch.nn.Linear(32,1)\n",
    "        \n",
    "    def forward(self,x,batch_input_anji):\n",
    "        outputs_ = self.esm2(**inputs)\n",
    "\n",
    "        x = outputs_.last_hidden_state[:, 0, :]\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        \n",
    "        fx_1 = self.fz_fc3(x)\n",
    "        fx_1 = self.sigmoid(fx_1)\n",
    "        \n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        batch_input_anji = self.fc_anji1(batch_input_anji)\n",
    "        batch_input_anji = self.relu(batch_input_anji)\n",
    "        batch_input_anji = self.fc_anji2(batch_input_anji)\n",
    "        batch_input_anji = self.relu(batch_input_anji)\n",
    "        \n",
    "        fx_2 = self.fz_fc1(batch_input_anji)\n",
    "        fx_2 = self.sigmoid(fx_2)\n",
    "        \n",
    "        batch_input_anji = self.dropout2(batch_input_anji)\n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        x = torch.cat((batch_input_anji, x), dim=1)\n",
    "\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        x = self.sigmoid(x)\n",
    "        return x,fx_1,fx_2\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4d772ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义测试集\n",
    "batch_size = 1  # 每个批次的样本数量\n",
    "val_data_loader = DataLoader(list(zip(test_feature, test_label)), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19649c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[    0     2     3 ... 13946 13947 13948]\n",
      "kf 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmModel were not initialized from the model checkpoint at facebook/esm2_t33_650M_UR50D and are newly initialized: ['esm.pooler.dense.bias', 'esm.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "import copy as cp\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import pandas as pd\n",
    "# 定义五折交叉验证\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "kf_in = 0\n",
    "\n",
    "feature = np.array(feature)\n",
    "label = np.array(label)\n",
    "# 迭代生成每一折的训练和测试数据索引\n",
    "for train_index, test_index in stratified_kfold.split(feature, label):\n",
    "    \n",
    "    save_model = None\n",
    "    best_accuracy = 0\n",
    "    best_f1 = 0\n",
    "    best_auc = 0\n",
    "    best_mcc = 0\n",
    "    best_recall = 0\n",
    "    best_precision = 0\n",
    "    \n",
    "\n",
    "    best_test_labels_com = None\n",
    "    best_predict_labels_com = None\n",
    "    best_test_outputs_com = None\n",
    "    best_intermediate_outputs = None\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(train_index)\n",
    "    kf_in += 1\n",
    "    print('kf',kf_in)\n",
    "    train_features, test_features = list(feature[train_index]), list(feature[test_index])\n",
    "    train_labels, test_labels = list(label[train_index]), list(label[test_index])\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 指定保存位置的路径\n",
    "    cache_directory = r\"C:\\Users\\Admin\\Desktop\\neuropeptide prediction\\model\"\n",
    "\n",
    "    # 加载 tokenizer 和模型\n",
    "    tokenizer_ = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\", cache_dir=cache_directory)\n",
    "    esm2 = AutoModel.from_pretrained(\"facebook/esm2_t33_650M_UR50D\", cache_dir=cache_directory)\n",
    "    esm2 = esm2.to(device)\n",
    "    del esm2.encoder.layer[4:]\n",
    "    \n",
    "    model = myModel(esm2)\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # 定义损失函数和优化器\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "    # 将数据包装成TensorDataset，并使用DataLoader进行批次训练\n",
    "    \n",
    "    data_loader = DataLoader(list(zip(train_features, train_labels)), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "    test_data_loader = DataLoader(list(zip(test_features,test_labels)), batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    \n",
    "    # 训练模型\n",
    "    num_epochs = 50\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        for batch_input, batch_labels in data_loader:\n",
    "        \n",
    "            batch_input_list = list(batch_input)\n",
    "            \n",
    "            batch_input_anji = []\n",
    "            for i in range(len(batch_input_list)):\n",
    "                batch_input_anji.append(getFre(batch_input_list[i]))\n",
    "                \n",
    "            for i in range(len(batch_input_list)):\n",
    "                kmer_counts = kmer_composition(batch_input_list[i], k)\n",
    "                feature_temp = [0 for i in range(400)]\n",
    "                for it in kmer_counts:\n",
    "                    feature_temp[kmer_counts_dict[it]] = kmer_counts[it]\n",
    "                batch_input_anji[i].extend(feature_temp)\n",
    "    \n",
    "            batch_input_anji = torch.tensor(batch_input_anji).float()\n",
    "            batch_input_anji = batch_input_anji.to(device)\n",
    "            \n",
    "                \n",
    "            inputs = tokenizer_(batch_input, return_tensors='pt', padding=True, truncation=True)\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = model(inputs,batch_input_anji)\n",
    "        \n",
    "            outputs1 = outputs[0].view(-1)\n",
    "            outputs2 = outputs[1].view(-1)\n",
    "            outputs3 = outputs[2].view(-1)\n",
    "            loss1 = criterion(outputs1.to('cpu'), batch_labels.to(torch.float32))\n",
    "            loss2 = criterion(outputs2.to('cpu'), batch_labels.to(torch.float32))\n",
    "            loss3 = criterion(outputs3.to('cpu'), batch_labels.to(torch.float32))\n",
    "            loss = loss1 + loss2*0.3 + loss3*0.3\n",
    "            # 反向传播和优化\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        # 在每个训练周期之后进行测试集性能评估\n",
    "        model.eval()  # 切换模型为评估模式\n",
    "        test_loss = 0\n",
    "        test_loss1 = 0\n",
    "        test_loss2 = 0\n",
    "        test_loss3 = 0\n",
    "        test_loss4 = 0\n",
    "        correct_predictions1= 0\n",
    "        correct_predictions2= 0\n",
    "        correct_predictions3= 0\n",
    "        correct_predictions4= 0\n",
    "        \n",
    "        test_labels_com = []\n",
    "        predict_labels_com = []\n",
    "        test_outputs_com = []\n",
    "        intermediate_outputs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for test_batch_input,test_batch_labels in test_data_loader:\n",
    "                \n",
    "                batch_input_list = list(test_batch_input)\n",
    "            \n",
    "                batch_input_anji = []\n",
    "                for i in range(len(batch_input_list)):\n",
    "                    batch_input_anji.append(getFre(batch_input_list[i]))\n",
    "\n",
    "                    \n",
    "                for i in range(len(batch_input_list)):\n",
    "                    kmer_counts = kmer_composition(batch_input_list[i], k)\n",
    "                    feature_temp = [0 for i in range(400)]\n",
    "                    for it in kmer_counts:\n",
    "                        feature_temp[kmer_counts_dict[it]] = kmer_counts[it]\n",
    "                    batch_input_anji[i].extend(feature_temp)\n",
    "                batch_input_anji = torch.tensor(batch_input_anji).float()\n",
    "                batch_input_anji = batch_input_anji.to(device)\n",
    "                \n",
    "\n",
    "                inputs = tokenizer_(test_batch_input, return_tensors='pt', padding=True, truncation=True)\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "                test_outputs = model(inputs,batch_input_anji)\n",
    "                \n",
    "        \n",
    "                test_outputs1 = test_outputs[0].view(-1)\n",
    "                test_outputs2 = test_outputs[1].view(-1)\n",
    "                test_outputs3 = test_outputs[2].view(-1)\n",
    "        \n",
    "                \n",
    "        \n",
    "                loss1 = criterion(test_outputs1.to('cpu'), test_batch_labels.to(torch.float32)).item()\n",
    "                loss2 = criterion(test_outputs2.to('cpu'), test_batch_labels.to(torch.float32)).item()\n",
    "                loss3 = criterion(test_outputs3.to('cpu'), test_batch_labels.to(torch.float32)).item()\n",
    "                test_loss1 += loss1\n",
    "                test_loss2 += loss2\n",
    "                test_loss3 += loss3\n",
    "                \n",
    "                test_loss += (loss1 + loss2*0.3 + loss3*0.3)\n",
    "                \n",
    "                predictions1 = (test_outputs1 > 0.5).float()  # 大于0.5的预测为1，否则为0\n",
    "                predictions2 = (test_outputs2 > 0.5).float()  # 大于0.5的预测为1，否则为0\n",
    "                predictions3 = (test_outputs3 > 0.5).float()  # 大于0.5的预测为1，否则为0\n",
    "                \n",
    "                correct_predictions1 += (predictions1.to('cpu') == test_batch_labels).sum().item()\n",
    "                correct_predictions2 += (predictions2.to('cpu') == test_batch_labels).sum().item()\n",
    "                correct_predictions3 += (predictions3.to('cpu') == test_batch_labels).sum().item()\n",
    "                \n",
    "                predict_labels_com.extend(predictions1.tolist())\n",
    "                test_labels_com.extend(test_batch_labels.tolist())\n",
    "                test_outputs_com.extend(test_outputs1.tolist())\n",
    "        \n",
    "        average_test_loss = test_loss / len(val_data_loader)\n",
    "        average_test_loss1 = test_loss1 / len(val_data_loader)\n",
    "        average_test_loss2 = test_loss2 / len(val_data_loader)\n",
    "        average_test_loss3 = test_loss3 / len(val_data_loader)\n",
    "        accuracy1 = correct_predictions1 / len(test_feature)\n",
    "        accuracy2 = correct_predictions2 / len(test_feature)\n",
    "        accuracy3 = correct_predictions3 / len(test_feature)\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        predict_labels_com = np.array(predict_labels_com)\n",
    "        test_labels_com = np.array(test_labels_com)\n",
    "        test_outputs_com = np.array(test_outputs_com)\n",
    "        temp_accuracy,temp_f1,temp_auc,temp_mcc,temp_recall,temp_precision = comp_result(test_labels_com,predict_labels_com,test_outputs_com)\n",
    "        if temp_accuracy > best_accuracy:\n",
    "            best_accuracy = temp_accuracy\n",
    "            best_f1 = temp_f1\n",
    "            best_auc = temp_auc\n",
    "            best_mcc = temp_mcc\n",
    "            best_recall = temp_recall\n",
    "            best_precision = temp_precision\n",
    "\n",
    "            \n",
    "            best_test_labels_com = test_labels_com\n",
    "            best_predict_labels_com = predict_labels_com\n",
    "            best_test_outputs_com = test_outputs_com\n",
    "            best_intermediate_outputs = intermediate_outputs\n",
    "            save_model = cp.deepcopy(model)\n",
    "            \n",
    "            temp_output_pd = pd.DataFrame(best_test_outputs_com)\n",
    "            temp_label_pd = pd.DataFrame(best_test_labels_com)\n",
    "\n",
    "            temp_output_pd.to_csv(r'C:\\Users\\Admin\\Desktop\\self trans\\result\\new_Fold%d_output.csv' % kf_in)\n",
    "            temp_label_pd.to_csv(r'C:\\Users\\Admin\\Desktop\\self trans\\result\\new_Fold%d_label.csv' % kf_in)\n",
    "        \n",
    "        average_train_loss = total_loss / len(data_loader)\n",
    "        \n",
    "        \n",
    "        # 测试集\n",
    "        print(\"测试集================\")\n",
    "        test_labels_com = []\n",
    "        predict_labels_com = []\n",
    "        test_outputs_com = []\n",
    "        intermediate_outputs = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for test_batch_input, test_batch_labels in val_data_loader:\n",
    "                \n",
    "                batch_input_list = list(test_batch_input)\n",
    "            \n",
    "                batch_input_anji = []\n",
    "                for i in range(len(batch_input_list)):\n",
    "                    batch_input_anji.append(getFre(batch_input_list[i]))\n",
    "\n",
    "                    \n",
    "                for i in range(len(batch_input_list)):\n",
    "                    kmer_counts = kmer_composition(batch_input_list[i], k)\n",
    "                    feature_temp = [0 for i in range(400)]\n",
    "                    for it in kmer_counts:\n",
    "                        feature_temp[kmer_counts_dict[it]] = kmer_counts[it]\n",
    "                    batch_input_anji[i].extend(feature_temp)\n",
    "                batch_input_anji = torch.tensor(batch_input_anji).float()\n",
    "                batch_input_anji = batch_input_anji.to(device)\n",
    "                \n",
    "\n",
    "                inputs = tokenizer_(test_batch_input, return_tensors='pt', padding=True, truncation=True)\n",
    "                inputs = inputs.to(device)\n",
    "\n",
    "                test_outputs = model(inputs,batch_input_anji)\n",
    "                \n",
    "        \n",
    "                test_outputs1 = test_outputs[0].view(-1)\n",
    "       \n",
    "                predictions1 = (test_outputs1 > 0.5).float()  # 大于0.5的预测为1，否则为0\n",
    "                \n",
    "                \n",
    "                predict_labels_com.extend(predictions1.tolist())\n",
    "                test_labels_com.extend(test_batch_labels.tolist())\n",
    "                test_outputs_com.extend(test_outputs1.tolist())\n",
    "\n",
    "        predict_labels_com = np.array(predict_labels_com)\n",
    "        test_labels_com = np.array(test_labels_com)\n",
    "        test_outputs_com = np.array(test_outputs_com)\n",
    "        comp_result(test_labels_com,predict_labels_com,test_outputs_com)\n",
    "        print(\"======================\")\n",
    "        # 切回训练模式\n",
    "        model.train()\n",
    "        print('Epoch [{}/{}], Train Loss: {:.4f}, Test Loss: {:.4f}, Test Loss1: {:.4f}, Test Loss2: {:.4f}, Test Loss3: {:.4f}, Accuracy1: {:.2f}, Accuracy2: {:.2f}, Accuracy3: {:.2f}'.format(\n",
    "            epoch + 1, num_epochs, average_train_loss, average_test_loss, average_test_loss1, average_test_loss2, average_test_loss3, accuracy1 * 100, accuracy2 * 100, accuracy3 * 100))\n",
    "    torch.save(save_model, r'C:\\Users\\Admin\\Desktop\\self trans\\reuslt_model\\new_best_model_%d.pth' % kf_in)\n",
    "    print('=====================================')\n",
    "    print('kf',kf_in,\"best result:\")\n",
    "    print('acc: {:.4f}'.format(best_accuracy))\n",
    "    print('f1: {:.4f}'.format(best_f1))\n",
    "    print('auc： {:.4f}'.format(best_auc))\n",
    "    print('mcc: {:.4f}'.format(best_mcc))\n",
    "    print('recall：{:.4f}'.format(best_recall))\n",
    "    print('prec: {:.4f}'.format(best_precision))\n",
    "    print('=====================================')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
